{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_model end\n",
      "lsi_model end\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora,models, similarities\n",
    "import logging\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "with open ('questions', 'rb') as fp:\n",
    "    questionList = pickle.load(fp)\n",
    "questionList = list(set(questionList))\n",
    "questionList = [x.replace('How to ', '') for x in questionList] \n",
    "\n",
    "def save_pickle(file_name,file_save):\n",
    "    file = open(file_name, 'wb')\n",
    "    pickle.dump(file_save, file)\n",
    "    file.close()\n",
    "\n",
    "def token(question):\n",
    "    word_token = words.findall(question)\n",
    "#     lower = [stemmer.stem(i.lower()) for i in word_token if i not in stopword]  # 要加[],否则for出错\n",
    "    lower = [(i.lower()) for i in word_token if i not in stopword]\n",
    "    return lower\n",
    "\n",
    "def get_data_token():\n",
    "    comparison_token = []\n",
    "    for text in questionList:\n",
    "        word_list = token(text)\n",
    "        l = len(word_list)\n",
    "        word_list[l - 1] = word_list[l - 1].strip()\n",
    "        comparison_token.append(word_list)\n",
    "\n",
    "    return comparison_token \n",
    "    \n",
    "comparison_token = get_data_token()\n",
    "save_pickle('comparison_token',comparison_token)\n",
    "\n",
    "################################## tfidf #######################################\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "dictionary = corpora.Dictionary(comparison_token)\n",
    "dictionary.save(\"wikihow_dictionary.dic\")   #save dic \n",
    "\n",
    "bow = dictionary.token2id\n",
    "corpus = [dictionary.doc2bow(text) for text in comparison_token]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf.save(\"wikihow_TFIDF_model.mdl\")    \n",
    "tfidf_corpus = tfidf[corpus]\n",
    "indexTfidf = similarities.MatrixSimilarity(tfidf_corpus)\n",
    "indexTfidf.save(\"wikihow_TFIDF.idx\")\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary.load(\"wikihow_dictionary.dic\")\n",
    "tfidfModel = models.TfidfModel.load(\"wikihow_TFIDF_model.mdl\")\n",
    "indexTfidf = similarities.MatrixSimilarity.load(\"wikihow_TFIDF.idx\")\n",
    "print('tfidf_model end')\n",
    "\n",
    "################################## lsi run model ###############################\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=50)\n",
    "lsi_model.save(\"wikihow_Lsi50Topic.mdl\")\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi_model[corpus])\n",
    "indexTfidf.save(\"wikihow_lsi.idx\")\n",
    "print('lsi_model end')\n",
    "\n",
    "############################   kl-divergence    #################################\n",
    "import abc\n",
    "import collections\n",
    "class LanguageModel(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, vocab, order):\n",
    "        self.vocab = vocab\n",
    "        self.order = order\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def probability(self, word, *history):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class CountLM(LanguageModel):\n",
    "    @abc.abstractmethod\n",
    "    def counts(self, word_and_history):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def norm(self, history):\n",
    "        pass\n",
    "\n",
    "    def probability(self, word, *history):\n",
    "        if word not in self.vocab:\n",
    "            return 0.0\n",
    "        sub_history = tuple(history[-(self.order - 1):]) if self.order > 1 else ()\n",
    "        norm = self.norm(sub_history)\n",
    "        if norm == 0:\n",
    "            return 1.0 / len(self.vocab)\n",
    "        else:\n",
    "            return self.counts((word,) + sub_history) / self.norm(sub_history)\n",
    "\n",
    "\n",
    "class NGramLM(CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float) \n",
    "        self._norm = collections.defaultdict(float)\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1: i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]\n",
    "class InterpolatedLM(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def probability(self, word, *history):\n",
    "        return self.alpha * self.main.probability(word, *history) + \\\n",
    "               (1.0 - self.alpha) * self.backoff.probability(word, *history)\n",
    "def lm(token):    \n",
    "    unigram = NGramLM(token,1)\n",
    "    bigram  = NGramLM(token,2)\n",
    "    interpolate = InterpolatedLM(bigram,unigram,0.6)\n",
    "    return interpolate\n",
    "\n",
    "def KL_model(t,c): \n",
    "    kl = 0.0\n",
    "    KL = {}\n",
    "    Px = lm(t)\n",
    "    Qx = lm(c)\n",
    "    for word in t:\n",
    "        if Qx.probability(word)!=0 and Px.probability(word)!=0:\n",
    "            kl +=  -(Px.probability(word) * \n",
    "                    np.log(Qx.probability(word)))          \n",
    "    return kl   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121853"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v end\n"
     ]
    }
   ],
   "source": [
    "############################# w2v & d2v run model ###############################\n",
    "def d2vec(sorted_d2v):\n",
    "    print('d2v simi top:')\n",
    "    e = []\n",
    "    for d2v in sorted_d2v[:10]:\n",
    "        w2v_top50 = questionList[d2v.id]\n",
    "        print(w2v_top50)\n",
    "        e.append(w2v_top50)\n",
    "    return e \n",
    "\n",
    "def get_dataset():\n",
    "    train = []\n",
    "    for i, text in enumerate(questionList):\n",
    "        word_list = token(text)\n",
    "        l = len(word_list)\n",
    "        word_list[l - 1] = word_list[l - 1].strip()\n",
    "        document = TaggededDocument(word_list, tags=[i])\n",
    "        train.append(document)\n",
    "\n",
    "    return train\n",
    "\n",
    "def w2v_train(token):  ##以下步骤中可以直接load model\n",
    "#     token_wikihow =  pd.Series(token.tolist())\n",
    "    vec_wikihowall = Word2Vec(token, size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "    vec_wikihowall.save('wikihow_w2vec_model') ## change names \n",
    "#     model_vec_wikihow = gensim.models.Word2Vec.load('wikihow_w2vec_model') \n",
    "    return vec_wikihowall\n",
    "\n",
    "def represention(token,vec): ## token_train/test_head/body \n",
    "    represention = np.zeros((len(token), vec.vector_size))\n",
    "    for i, tokens in enumerate(token):\n",
    "        tokens = [t for t in tokens if t in vec.wv.vocab]  #加[] \n",
    "        if tokens:\n",
    "            represention[i, :] = np.mean([vec.wv[t] / vec.wv.vocab[t].count for t in tokens], axis=0)\n",
    "    return represention \n",
    "\n",
    "vec_wikihowall = w2v_train(comparison_token)\n",
    "vec_wikihowall = gensim.models.Word2Vec.load('wikihow_w2vec_model') \n",
    "# rep_comparison = represention(comparison_token,vec_wikihowall)\n",
    "# np.savetxt('w2v_vec_wikihowall.txt',rep_comparison,delimiter=',')\n",
    "print(\"w2v end\") \n",
    "\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "def d2v_train(x_train, size=200, epoch_num=1):\n",
    "    model_dm = Doc2Vec(x_train,min_count=1, window = 3, vector_size = size, sample=1e-3, negative=5, workers=4)\n",
    "    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=70)\n",
    "    model_dm.save('wikihow_doc2vec_model')\n",
    " \n",
    "    return model_dm \n",
    "\n",
    "def get_dataset():\n",
    "    train = []\n",
    "    for i, text in enumerate(questionList):\n",
    "        word_list = token(text)\n",
    "        l = len(word_list)\n",
    "        word_list[l - 1] = word_list[l - 1].strip()\n",
    "        document = TaggededDocument(word_list, tags=[i])\n",
    "        train.append(document)\n",
    "\n",
    "    return train\n",
    "\n",
    "# d2v_token = get_dataset()\n",
    "# model_dm = d2v_train(d2v_token, size=200, epoch_num=1)\n",
    "# print(\"d2v end\") \n",
    "\n",
    "############################# w2v & d2v run model ###############################\n",
    "model = \"wikihow_w2vec_model\" \n",
    "# model_d = 'wikihow_doc2vec_model'\n",
    "\n",
    "model_w2v = Word2Vec.load(model)\n",
    "# model_d2v = Doc2Vec.load(model_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_model: w2v, tfidf, lsi, kl ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(ques):\n",
    "    class ResultInfo(object):\n",
    "        def __init__(self, index, score,score_d2v, text):\n",
    "            self.id = index\n",
    "            self.score = score\n",
    "            self.score_d2v = score_d2v\n",
    "            self.text = text    \n",
    "\n",
    "    ###############################################################################################\n",
    "    target= ques\n",
    "    query = token(target)\n",
    "    \n",
    "    for w in query:\n",
    "        if w not in model_w2v.wv.vocab:\n",
    "            print (\"input word %s not in dict. skip this turn\" % w)\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    score_d2v = 0\n",
    "    for comparison in comparison_token:    \n",
    "        #         print (comparisons)\n",
    "        score = model_w2v.wv.n_similarity(query, comparison)\n",
    "        score_d2v = model_d2v.docvecs.similarity_unseen_docs(model_d2v, query, comparison)\n",
    "        res.append(ResultInfo(index, score, score_d2v,\" \".join(comparison)))\n",
    "        index += 1\n",
    "\n",
    "    sorted_w2v = sorted(res, key=lambda ResultInfo:ResultInfo.score, reverse=True)\n",
    "    print('sorted_w2v')\n",
    "#     sorted_d2v = sorted(res, key=lambda ResultInfo:ResultInfo.score_d2v, reverse=True)\n",
    "#     print('sorted_d2v')       \n",
    "    return sorted_w2v\n",
    "\n",
    "def w2v_model():\n",
    "    sorted_w2v = w2v()\n",
    "    return d2vec(sorted_w2v)\n",
    "\n",
    "def w2vec(sorted_w2v): \n",
    "    print('w2v simi top:') \n",
    "    d = []\n",
    "    for w in sorted_w2v[:100]:\n",
    "        w2v_top50 = questionList[w.id]\n",
    "#         print(w2v_top50)\n",
    "        d.append(w2v_top50)      \n",
    "    return d  \n",
    "\n",
    "def tfidf(ques):\n",
    "    target = ques\n",
    "    query_bow = dictionary.doc2bow(token(target))\n",
    "#     print(query)\n",
    "    tfidfvect = tfidfModel[query_bow]\n",
    "    simstfidf = indexTfidf[tfidfvect]\n",
    "\n",
    "    tfidf_sims = sorted(enumerate(simstfidf), key=lambda item: -item[1])\n",
    "    print (\"TFIDF similary Top:\") \n",
    "    a = []\n",
    "    for sim in tfidf_sims[:100]:\n",
    "        tfidf_top50 = questionList[sim[0]]\n",
    "#         print(tfidf_top50)\n",
    "        a.append(tfidf_top50)\n",
    "    return a  \n",
    "\n",
    "def lsi(ques):\n",
    "    target = ques\n",
    "    bow = dictionary.doc2bow(token(target))\n",
    "    lsi_represent= lsi_model[bow]\n",
    "    \n",
    "    sims = index[lsi_represent]\n",
    "\n",
    "    lsi_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    print (\"lsi similary Top:\")\n",
    "    b = []\n",
    "    for sim in lsi_sims[:100]:\n",
    "        lsi_top50 = questionList[sim[0]]\n",
    "#         print(lsi_top50)   \n",
    "        b.append(lsi_top50)\n",
    "    return b \n",
    "\n",
    "def kl(ques):\n",
    "    class Resultkl(object):\n",
    "        def __init__(self, index, kl_score, text):\n",
    "            self.id = index\n",
    "            self.kl_score = kl_score\n",
    "            self.text = text\n",
    "    target = ques\n",
    "    query = token(target) \n",
    "    ###############################################################################################\n",
    "    res = []\n",
    "    index = 0\n",
    "    for comparison in comparison_token:\n",
    "    #         print (comparisons)\n",
    "        kl_score = KL_model(query, comparison)\n",
    "        res.append(Resultkl(index, kl_score,\" \".join(comparison)))\n",
    "        index += 1 \n",
    "        \n",
    "    sorted_kl = sorted(res, key=lambda Resultkl:Resultkl.kl_score, reverse=True)\n",
    "    c =[]\n",
    "    print (\"kl similary Top :\")\n",
    "    for i in sorted_kl[:100]:\n",
    "    #     print (\"text %s: %s, kl_score : %s \" % (i.id, i.text, i.kl_score))\n",
    "        kl_top = i.text\n",
    "#         print(kl_top)\n",
    "        c.append(kl_top) \n",
    "    return c        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from simi --- topic model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simi(ques):\n",
    "    TFIDF = tfidf(ques)\n",
    "    LSI = lsi(ques)\n",
    "    KL = kl(ques)\n",
    "#     W2V = w2v_model()\n",
    "    mylist = TFIDF+LSI+KL\n",
    "    a = {}\n",
    "    for i in mylist:\n",
    "        a[i] = mylist.count(i)\n",
    "    sorted_x = sorted(a.items(), key=lambda d:d[1], reverse = True)\n",
    "    c = []\n",
    "    for info in sorted_x:\n",
    "        c.append(info[0])\n",
    "    b_set = c[:6]\n",
    "    print(len(c))\n",
    "    return b_set\n",
    "\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    c = {}\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "#         print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort(W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        a = []\n",
    "        for doc_index in top_doc_indices:\n",
    "            a.append(documents[doc_index])\n",
    "        c[topic_idx] = a\n",
    "    print(c)\n",
    "    return list(c.values())\n",
    "\n",
    "def topic_model(b_set):\n",
    "    documents = b_set\n",
    "\n",
    "    no_features = 1000\n",
    "\n",
    "    # NMF is able to use tf-idf\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "    # LDA can only use raw term counts 只考虑每种词汇在该训练文本中出现的频率\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    no_topics = 4\n",
    "\n",
    "    # Run NMF\n",
    "    nmf_model = NMF(n_components=no_topics, random_state=None, alpha=.1, l1_ratio=.5, init='nndsvd',shuffle=False).fit(tfidf)\n",
    "    nmf_W = nmf_model.transform(tfidf,**params)\n",
    "    nmf_H = nmf_model.components_\n",
    "#     n_components=None, init=None, solver=’cd’, beta_loss=’frobenius’, tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, verbose=0, shuffle=False\n",
    "\n",
    "\n",
    "\n",
    "    # Run LDA\n",
    "    lda_model = LatentDirichletAllocation(n_components=no_topics, \n",
    "         max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "    lda_W = lda_model.transform(tf)\n",
    "    lda_H = lda_model.components_\n",
    "\n",
    "    no_top_words = 10\n",
    "    no_top_documents = 2\n",
    "    return display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, \n",
    "                   no_top_words, no_top_documents)\n",
    "\n",
    "def lda_model(b_set):\n",
    "    documents = b_set\n",
    "\n",
    "    no_features = 1000\n",
    "\n",
    "#     # NMF is able to use tf-idf\n",
    "#     tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "#     tfidf1 = tfidf_vectorizer.fit_transform(documents)\n",
    "#     tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # LDA can only use raw term counts 只考虑每种词汇在该训练文本中出现的频率\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    no_topics = 4\n",
    "\n",
    "#     # Run NMF\n",
    "#     nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1,l1_ratio=.5, init='nndsvd').fit(tfidf1)\n",
    "#     nmf_W = nmf_model.transform(tfidf1)\n",
    "#     nmf_H = nmf_model.components_\n",
    "\n",
    "    # Run LDA\n",
    "    lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "    lda_W = lda_model.transform(tf)\n",
    "    lda_H = lda_model.components_\n",
    "\n",
    "    no_top_words = 10\n",
    "    no_top_documents = 1\n",
    "    return  display_topics(lda_H, lda_W, tf_feature_names, documents, \n",
    "                   no_top_words, no_top_documents)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终模型1 - 找相似问题 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sim(ques):\n",
    "    b_set = simi(ques) \n",
    "#     nmf_questions = topic_model(b_set)\n",
    "    lda_questions = lda_model(b_set)\n",
    "    return lda_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终模型2 - 归纳答案##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Cope in a High Pressure Customer Service Role\n",
      "https://www.wikihow.com/Cope-in-a-High-Pressure-Customer-Service-Role\n",
      "step  1 :  Try to keep your cool when dealing with an irate customer.\n",
      "step  2 :  Set achievable targets.\n",
      "step  3 :  Don't let other staff members get you down.\n",
      "step  4 :  Leave your work behind at the end of the day.\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "with open('questions','rb') as fp:\n",
    "    comparison = pickle.load(fp)\n",
    "    comparisons = list(set(comparison))\n",
    "\n",
    "target = (choice(comparisons))\n",
    "\n",
    "target2 = target.replace('How to ', '')\n",
    "Target = target2.replace(\" \", \"-\")\n",
    "print (target) \n",
    "\n",
    "base_url = 'https://www.wikihow.com/'\n",
    "url= urljoin(base_url,Target)\n",
    "print(url)\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "# print(html)\n",
    "##############################################################\n",
    "all_href = soup.find_all('h3')\n",
    "# print (all_href)\n",
    "count = 0\n",
    "partTitle = []\n",
    "for l in all_href:\n",
    "#     print('\\n', all_href)\n",
    "    jan = l.find_all('span', {\"class\": 'mw-headline'})\n",
    "    for d in jan:\n",
    "        count += 1\n",
    "#         print(\"part \", count,\": \", d.get_text())\n",
    "        partTitle.append(d.get_text())\n",
    "##############################################################\n",
    "if partTitle != []:\n",
    "    href = soup.find_all(class_ = 'section_text')\n",
    "    count = 1\n",
    "    length = len(partTitle)\n",
    "    for l in href:\n",
    "        if(count == length+1):\n",
    "            break\n",
    "        print(\"\\nPart \" + str(count) + \": \" + partTitle[count-1])\n",
    "        month = l.find_all('b', {\"class\": \"whb\"})\n",
    "        for m in month:\n",
    "            print(m.get_text())\n",
    "        count += 1\n",
    "    \n",
    "else:\n",
    "    href = soup.find_all(class_ = 'section_text')\n",
    "    count = 0\n",
    "    for l in href:\n",
    "        month = l.find_all('b', {\"class\": \"whb\"})\n",
    "        for m in month:\n",
    "            count += 1\n",
    "            print(\"step \", count,\": \", m.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tkinter ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF similary Top:\n",
      "lsi similary Top:\n",
      "kl similary Top :\n",
      "271\n",
      "TFIDF similary Top:\n",
      "lsi similary Top:\n",
      "kl similary Top :\n",
      "271\n",
      "TFIDF similary Top:\n",
      "lsi similary Top:\n",
      "kl similary Top :\n",
      "291\n",
      "TFIDF similary Top:\n",
      "lsi similary Top:\n",
      "kl similary Top :\n",
      "285\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "def find_ques(ques=\"\"):\n",
    "    ques=query_entry.get()\n",
    "    ans_text.delete(1.0, tk.END)\n",
    "    \n",
    "    sim_list=find_sim(ques)\n",
    "    lb.delete(0,tk.END)\n",
    "    for simq in sim_list:\n",
    "        lb.insert(tk.END,simq)\n",
    "\n",
    "def get_ans(target): \n",
    "#     target2 = target.replace('How to ', '')\n",
    "    target1 = target.__str__()\n",
    "    Target = target1.replace(\" \", \"-\")\n",
    "    # print (Target)\n",
    "\n",
    "    base_url = 'https://www.wikihow.com/'\n",
    "    url= urljoin(base_url,Target)\n",
    "    # print(url)\n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    # soup = BeautifulSoup(html, features='lxml')\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # print(html)\n",
    "    ##############################################################\n",
    "    all_href = soup.find_all('h3')\n",
    "    # print (all_href)\n",
    "    count = 0\n",
    "    partTitle = []\n",
    "    for l in all_href:\n",
    "    #     print('\\n', all_href)\n",
    "        jan = l.find_all('span', {\"class\": 'mw-headline'})\n",
    "        for d in jan:\n",
    "            count += 1\n",
    "    #         print(\"part \", count,\": \", d.get_text())\n",
    "            partTitle.append(d.get_text())\n",
    "    ##############################################################\n",
    "    res_string=\"\"\n",
    "    if partTitle != []:\n",
    "        href = soup.find_all(class_ = 'section_text')\n",
    "        count = 1\n",
    "        length = len(partTitle)\n",
    "        for l in href:\n",
    "            if(count == length+1):\n",
    "                break\n",
    "            # print(\"\\nPart \" + str(count) + \": \" + partTitle[count-1])\n",
    "            res_string +=\"\\nPart \" + str(count) + \": \" + partTitle[count-1]\n",
    "            month = l.find_all('b', {\"class\": \"whb\"})\n",
    "            for m in month:\n",
    "                # print(m.get_text())\n",
    "                res_string +=m.get_text()\n",
    "            count += 1\n",
    "        return url,res_string\n",
    "\n",
    "    else:\n",
    "        href = soup.find_all(class_ = 'section_text')\n",
    "        count = 0\n",
    "        for l in href:\n",
    "            month = l.find_all('b', {\"class\": \"whb\"})\n",
    "            for m in month:\n",
    "                count += 1\n",
    "                # print(\"step \", count,\": \", m.get_text())\n",
    "                res_string +=\"step \"+ str(count)+\": \"+m.get_text()\n",
    "        return url,res_string\n",
    "    \n",
    "def show_ans(event):\n",
    "    if lb.curselection():\n",
    "        sim_q=lb.get(lb.curselection())\n",
    "\n",
    "        query_entry.delete(0, tk.END)\n",
    "        query_entry.insert(10, sim_q)\n",
    "\n",
    "        answer = get_ans(sim_q)\n",
    "        ans_text.delete(1.0, tk.END)\n",
    "        if answer is not None:\n",
    "            ans_text.insert(tk.END, answer)\n",
    "\n",
    "# def find_sim(ques):\n",
    "#     b_set = simi(ques) \n",
    "# #     nmf_questions = topic_model(b_set)\n",
    "#     lda_questions = lda_model(b_set)\n",
    "# #     a = nmf_questions,lda_questions\n",
    "#     return lda_questions\n",
    "\n",
    "def find_sim(ques):\n",
    "    b_set = simi(ques)\n",
    "    return b_set[:6] \n",
    "    \n",
    "root=tk.Tk()\n",
    "root.geometry('600x600')\n",
    "root.title(\"Related 'How to' questions and answer finder\")\n",
    "\n",
    "tk.Label(root,text=\"Enter your Query：\",font=(\"Arial Bold\", 15)).grid(row=0,column=0)\n",
    "\n",
    "query_entry=tk.Entry(root,width=40)\n",
    "query_entry.grid(row=0,column=1)\n",
    "\n",
    "find_bt=tk.Button(root,text=\"Find\",bg=\"red\",command=find_ques)\n",
    "find_bt.grid(row=0,column=2)\n",
    "\n",
    "canv=tk.Canvas(root,height=15,width=370)\n",
    "canv.create_line(5,5,350,5,fill=\"blue\")\n",
    "canv.grid(row=1,columnspan=3)\n",
    "\n",
    "ans_text=scrolledtext.ScrolledText(root,height=20,width=80,bd=2,bg=\"PaleGoldenrod\")\n",
    "\n",
    "# ans_text.insert(tk.END,txt)\n",
    "ans_text.grid(padx=5,row=7,columnspan=3)\n",
    "\n",
    "canv=tk.Canvas(root,height=15,width= 370)\n",
    "canv.create_line(5,5,350,5,fill=\"blue\")\n",
    "canv.grid(row=5,columnspan=3)\n",
    "\n",
    "tk.Label(root,\n",
    "         text='Related Questions box:',\n",
    "         font=(\"Arial Bold\", 12)).grid(row=2,column=0)\n",
    "\n",
    "tk.Label(root,\n",
    "         text='Double Click the related questions, the answer will display in the \"Answer box\" ',\n",
    "         font=(\"bold italic\", 12)).grid(row=4,columnspan=3)\n",
    "tk.Label(root,\n",
    "         text='Answer box:',\n",
    "         font=(\"Arial Bold\", 12)).grid(row=6,column=0)\n",
    "\n",
    "lb=tk.Listbox(root,height=8,width=60)\n",
    "lb.grid(row=3,columnspan=3)\n",
    "lb.bind(\"<Button-1>\",show_ans)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
